from cassandra import ConsistencyLevel
from cassandra.cluster import Cluster, Session
from cassandra.auth import PlainTextAuthProvider
import json
import pandas as pd
import ast
import time
import query_generation
from datetime import datetime
from cassandra.query import SimpleStatement

CREDENTIALS_PATH = "credentials/"
DATASET_PATH = "dataset/"
SAT_QUERY = (
    "SELECT table_name FROM system_schema.tables WHERE keyspace_name = 'recipes'"
)


def connectToDB(name="paris"):
    path = CREDENTIALS_PATH + name + "_"
    cloud_config = {
        "secure_connect_bundle": path + "secure-connect-big-data-project-db.zip"
    }

    # This token JSON file is autogenerated when you download your token,
    # if yours is different update the file name below
    with open(path + "big_data_project_db-token.json") as f:
        secrets = json.load(f)

    CLIENT_ID = secrets["clientId"]
    CLIENT_SECRET = secrets["secret"]

    auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
    cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
    session = cluster.connect()

    row = session.execute("select release_version from system.local").one()
    if row:
        print("Connection successful!")
        session.set_keyspace("recipes")
        return session
    else:
        print("An error occurred.")
        return -1


def createTables(session: Session):
    """Creates all tables in the database."""

    create_table_queries = query_generation.getAllCreateTableQueries()
    [session.execute(q) for q in create_table_queries]
    print("Created tables!")


def split_name(name):
    return name.split()


def assign_difficulty(row, tertiles):
    if row["minutes"] <= tertiles[0]:
        return "Easy"
    elif row["minutes"] <= tertiles[1]:
        return "Medium"
    else:
        return "Hard"


def mergeDataframes():
    recipes_df = pd.read_csv(DATASET_PATH + "sample_RAW_recipes.csv")
    interactions_df = pd.read_csv(DATASET_PATH + "sample_RAW_interactions.csv")
    recipes_df.fillna({"name": "", "description": ""}, inplace=True)

    # Convert string representations of lists back to actual lists
    for column in recipes_df.columns:
        try:
            recipes_df[column] = recipes_df[column].apply(ast.literal_eval)
        except (ValueError, SyntaxError):
            pass  # Skip columns that cannot be converted to lists

    recipes_df["keywords"] = recipes_df["name"].apply(split_name)

    tertiles = recipes_df["minutes"].quantile([1 / 3, 2 / 3]).tolist()

    # Assign difficulty based on tertiles-
    recipes_df["difficulty"] = recipes_df.apply(
        lambda row: assign_difficulty(row, tertiles), axis=1
    )

    merged_df = pd.merge(
        recipes_df,
        interactions_df[["recipe_id", "rating"]]
        .groupby(by="recipe_id")
        .mean()
        .rename(columns={"rating": "avg_rating"}),
        left_on="id",
        right_on="recipe_id",
    )
    return merged_df


def insertData(session: Session):
    merged_df = mergeDataframes()
    (dataframes, queries) = query_generation.getAllInsertQueries(merged_df)

    for index, query in enumerate(queries):
        print(query)
        insert_query = session.prepare(query)
        df = dataframes[index]
        total_rows = len(df)
        values = df.values.tolist()
        start_time = time.time()
        for idx, row in enumerate(values, start=1):
            session.execute(insert_query, row)
            # Print out the progress and estimated time of completion
            if idx % 100 == 0:
                elapsed_time = time.time() - start_time
                rows_per_second = idx / elapsed_time
                estimated_total_time = total_rows / rows_per_second
                remaining_time = estimated_total_time - elapsed_time
                print(
                    f"Inserted {idx} of {total_rows} records ({(idx/total_rows)*100:.2f}%)"
                )
                print(f"Estimated time remaining: {remaining_time/60:.2f} minutes")
    print("Data loading complete.")


def insertDataWithConsistency(session: Session, consistency_level):
    df = mergeDataframes()
    (dataframes, queries) = query_generation.getAllInsertQueries(df)
    times = []

    for index, query in enumerate(queries):
        print(query)
        insert_query = session.prepare(query)
        insert_query.consistency_level = consistency_level
        df = dataframes[index]
        total_rows = len(df)
        values = df.values.tolist()
        start_time = time.time()
        for idx, row in enumerate(values, start=1):
            session.execute(insert_query, row)
            # Print out the progress and estimated time of completion
            if idx % 1_000 == 0:
                elapsed_time = time.time() - start_time
                rows_per_second = idx / elapsed_time
                estimated_total_time = total_rows / rows_per_second
                remaining_time = estimated_total_time - elapsed_time
                print(
                    f"Inserted {idx} of {total_rows} records ({(idx/total_rows)*100:.2f}%)"
                )
                print(f"Estimated time remaining: {remaining_time/60:.2f} minutes")
        elapsed_time = time.time() - start_time
        times.append(elapsed_time)
    avg_time = sum(times) / len(times) if times else 0
    print(f"Average insert time with {consistency_level}: {avg_time:.2f} seconds.")
    return avg_time


def dropAllTables(session: Session):
    result = session.execute(SAT_QUERY)
    tables = [row.table_name for row in result]

    # Drop each table
    for table in tables:
        drop_query = f"DROP TABLE IF EXISTS recipes.{table}"
        session.execute(drop_query)
        print(f"Dropped table: {table}")


def printAllTablesLength(session: Session):
    result = session.execute(SAT_QUERY)
    tables = [row.table_name for row in result]
    for table in tables:
        select_query = f"SELECT id FROM {table}"
        ans = session.execute(select_query)._current_rows
        print(f"Table {table} has {len(ans)} rows.")


def decryptTimestamp(timestamp):
    return datetime.utcfromtimestamp(timestamp).strftime("%Y-%m-%d")


def loadDataIntoDataframe(recipes: object):
    data = []
    columns = recipes.column_names
    data = [dict(zip(columns, row)) for row in recipes]
    return pd.DataFrame(data)


def executeSelectQueries(session: Session, consistency_level, queries=[]):
    start_time = time.time()
    for query_text in queries:
        for _ in range(10):  # Run each select 10 times
            query = SimpleStatement(query_text, consistency_level=consistency_level)
            session.execute(query)
    elapsed_time = time.time() - start_time
    avg_time = elapsed_time / (len(queries) * 10)

    # Find the name of the consistency level for printing
    consistency_name = [
        name
        for name, value in ConsistencyLevel.__dict__.items()
        if value == consistency_level
    ][0]
    print(
        f"Average select time with {consistency_name} = {consistency_level}: {avg_time:.4f} seconds."
    )
    return avg_time


import pandas as pd
import time
from cassandra.query import SimpleStatement
from cassandra import ConsistencyLevel
from cassandra.cluster import Session


def executeSelectQueriesv2(
    session: Session, consistency_level, queries=[], df=None, query_idx=0
):
    start_time = time.time()
    if df is None:
        df = pd.DataFrame(
            columns=["Query_Number", "Iteration", "Time", "Consistency_Level"]
        )

    for _, query_text in enumerate(queries, start=1):
        for run in range(1, 11):  # Run each select 10 times
            query = SimpleStatement(query_text, consistency_level=consistency_level)
            query_number = f"Query_{query_idx + 1}"
            query_start_time = time.time()
            session.execute(query)
            query_elapsed_time = time.time() - query_start_time
            df = pd.concat(
                [
                    df,
                    pd.DataFrame(
                        [
                            {
                                "Query_Number": query_number,
                                "Iteration": run,
                                "Time": query_elapsed_time,
                                "Consistency_Level": consistency_level,
                            }
                        ]
                    ),
                ],
                ignore_index=True,
            )

    elapsed_time = time.time() - start_time
    avg_time = elapsed_time / (len(queries) * 10)

    # Find the name of the consistency level for printing
    consistency_name = [
        name
        for name, value in ConsistencyLevel.__dict__.items()
        if value == consistency_level
    ][0]
    print(
        f"Average select time with {consistency_name} = {consistency_level}: {avg_time:.4f} seconds."
    )

    return avg_time, df


# Example usage:
# df, avg_time = executeSelectQueries(session, ConsistencyLevel.ONE, ["SELECT * FROM table1", "SELECT * FROM table2"])
